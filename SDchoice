import os
import pandas as pd
import nltk
import torch
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset



#Parse file
def parse_cha_file(file_path):
    speaker_utterances = []

    with open(file_path, 'r') as file:
        for line in file:
            # Strip whitespace from the line
            line = line.strip()
            # Check if the line starts with a speaker identifier
            if line.startswith('*'):
                # Find the speaker identifier and the utterance
                speaker, utterance = line.split(':', 1)
                # Clean up speaker identifier (remove leading '*')
                speaker = speaker[1:]
                # Append the tuple (speaker, utterance) to the list
                speaker_utterances.append((speaker, utterance.strip()))
    
    # Create a DataFrame from the list of tuples
    df = pd.DataFrame(speaker_utterances, columns=['Speaker', 'Utterance'])
    return df

file_path = input("Enter Path to File: ")
df = parse_cha_file(file_path)



nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

demonstratives = ['this', 'that', 'these', 'those', 'here', 'there']

import re

def extract_demonstrative(Utterance, demonstratives):
    # Normalize the utterance by converting it to lowercase
    Utterance = Utterance.lower()
    
    # Check for each demonstrative and return the first one found
    for demonstrative in demonstratives:
        if re.search(r'\b' + re.escape(demonstrative) + r'\b', Utterance):
            return demonstrative
    return 'None'  # Return 'None' if no demonstrative is found

# Apply the function to the DataFrame
df['Demonstrative'] = df['Utterance'].apply(lambda x: extract_demonstrative(x, demonstratives))


def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    return ' '.join(filtered_tokens)

df['processed_context'] = df['Speaker'].apply(preprocess_text)


vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['processed_context'])

#Split data
y = df['Demonstrative']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


model = MultinomialNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))